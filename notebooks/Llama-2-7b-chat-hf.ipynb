{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a7a2be-74a8-4cf5-b4ea-da2383d6bf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc0a020746941a489b6afaf27bfb561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configure the model for 4-bit loading using bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,        # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,   # Use double quantization (improves performance)\n",
    "    bnb_4bit_quant_type='nf4',        # Use 'nf4' quantization (Non-Finite Quantization) for better precision\n",
    "    bnb_4bit_compute_dtype='float16'  # Use float16 to reduce memory\n",
    ")\n",
    "\n",
    "# 模型名称\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 使用低精度推理\n",
    "    device_map=\"auto\"  # 自动分配设备 (CPU 或 MPS)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3437f95f-0982-4846-b87b-739c2b48c8bf",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "input_text_list = [\n",
    "    \"全过程民主是什么？\",\n",
    "]\n",
    "\n",
    "# 模型推理\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    for input_text in input_text_list:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_length=50, \n",
    "            do_sample=True,  # 启用随机生成\n",
    "            top_k=50,  # 只考虑前50个可能的选项\n",
    "            top_p=0.95,  # 在前95%的概率中随机选择\n",
    "            temperature=0.9  # 控制输出的随机性\n",
    "        )\n",
    "    \n",
    "        # 解码输出\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68058a82-9ffe-4fef-bc15-edd0a6843d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's a summary of the article in 30 words or less:\n",
      "\n",
      "Mac App Store now offers iPhone & iPad apps to macOS users; developers can choose to make them available or not; existing users will be upgraded to macOS app.</s>"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "MAX_INPUT_TOKEN_LENGTH = 4096\n",
    "system_prompt = \"summarize in no more than 30 words\"\n",
    "chat_history = []\n",
    "message = \"\"\"\n",
    "iPhone and iPad apps will be made available on the Mac App Store to users running macOS 11 or later on Macs with Apple silicon, unless you edit your apps’ availability. There's no porting process since these apps use the same frameworks, resources, and runtime environments as they do on iOS and iPadOS.\n",
    "\n",
    "If your iOS app offers universal purchase and has an existing macOS platform, the option to offer the iOS app on the Mac App Store won’t be available. Additionally, if your iOS app is available on the Mac App Store by using this option and you later add a macOS app by adding the macOS platform in App Store Connect, releasing it to the store will replace the iOS app on the Mac App Store. All existing users for the iOS app will be updated to the macOS app.\n",
    "\n",
    "You can opt in and out from having your iPhone and iPad apps available on the Mac App Store for users running macOS 11 or later on Macs with Apple silicon at any point. This is set at the app level and will apply to all versions of your app.\n",
    "\n",
    "Apple automatically chooses the minimum macOS version required for compatibility, but you can select a different version when editing availability on an individual app basis. If an LSMinimumSystemVersion is set in your app, this selection will override it.\n",
    "\"\"\"\n",
    "\n",
    "conversation = []\n",
    "if system_prompt:\n",
    "    conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "conversation += chat_history\n",
    "conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "    input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "    gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=False)\n",
    "generate_kwargs = dict(\n",
    "    {\"input_ids\": input_ids},\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    temperature=0.6,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "\n",
    "t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "t.start()\n",
    "\n",
    "try:\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        print(text, end=\"\", flush=True)\n",
    "finally:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0534c-6811-4175-a1ca-9c22219c3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
